\documentclass[11 pt, twoside]{article}
\usepackage{textcomp}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{indentfirst} %Comment out for no first paragraph indent
\usepackage[parfill]{parskip}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{outlines}

\usepackage{fancyhdr}
\pagestyle{fancy}
\cfoot{\hyperlink{content}{\thepage}}
\lhead{}
\chead{}
\rfoot{}
\lfoot{}
\rhead{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


\usepackage{hyperref}
\hypersetup {
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\newcommand{\sepitem}{0pt} %Added room between items on the list, not including a list and its sublist
\newcommand{\seppar}{1pt} %Between items and lists overall

\setenumerate[1]{itemsep=\sepitem, parsep=\seppar}
\setenumerate[2]{itemsep=\sepitem, parsep=\seppar}
\setenumerate[3]{itemsep=\sepitem, parsep=\seppar}
\setenumerate[4]{itemsep=\sepitem, parsep=\seppar}

\newenvironment{outline*}
{
	\begin{outline}[enumerate]
	}
	{\end{outline}
}

\newcommand{\foot}[1]{\hyperlink{#1}{$_#1$}}

\begin{document}

\title{Linear Algebra and Ordinary Differential Equations}
\author{Avery Karlin}
\date{Fall 2016}
\newcommand{\textbook}{Differential Equations and Linear Algebra by Stephen Goode and Scott Annin}
\newcommand{\teacher}{Dr. Robert Powers}

\maketitle
\newpage
\hypertarget{content}{\tableofcontents}
\vspace{11pt}
\noindent
\underline{Primary Textbook}: \textbook\\
\underline{Secondary Textbook}: Paul's Calc IV Notes\\
\underline{Secondary Textbook}: Linear Algebra with Applications by Otto Bretscher
\underline{Teacher}: \teacher
\newpage

``Matrices give you the courage to do calculations you may not be able to do otherwise, representing a large amount of data as a single letter''

\section{Chapter 1 (Paul) - Basic Concepts}
\subsection{Definitions}
\begin{outline*}
\1 Differential equations are those which contain either ordinary or partial derivatives, such that it is an ordinary differential equation if it only contains the former, and a partial derivative equation if it contains the latter
\1 The order of an equation is given by the highest order derivative within it
\1 Linear differential equations are those which can be written in the form: $f(x) = a_n(x)y^{(n)}(x) + a_{n-1}(x)y^{(n-1)}(x) +... + a_1(x)y'(x) + a_0(x)y(x)$, where a(x) are coefficient functions or constants, while y(x) is the main function
\2 Linear differential equations have no power on the main function or any of its derivatives
\2 Nonlinear differential equations are those which cannot be written in this format
\1 The solution on an open interval interval is a function y(x) which satisfies the equation on the interval
\2 The interval restricts a variable value to prevent there from being issues with the solution, such as division by zero
\1 Initial conditions are a set of conditions on the solution to determine which solution is the correct one, since there may be infinite possible solutions
\2 Initial conditions are a value of y(x) or some degree of its derivation for some number
\2 Not all equations will have any solutions, such that the existence question is if a solution exists or not, and even if there is a solution, it may not be possible to find the solution
\2 The number of solutions to the equation, and the conditions needed to obtain a single equation are called the uniqueness question
\1 Initial value problems are those with an appropriate number of initial conditions, based on the order of the equation (equal to the order of the equation)
\1 The interval of validity is the largest possible continuous interval in an initial value problem, such that the solution is valid and contains the value of x within the initial conditions
\1 The general solution is the the most general form a solution can take, ignoring initial conditions
\1 The actual solution is the solution which satisfies the equation and initial conditions
\1 An explicit solution is a solution in the form of y = y(x), such that y only appears on one side, without any exponents, while an implicit solution is non-explicit
\2 Both the actual and general solution may exist in both forms, though all explicit forms of the implicit solution may not be real solutions
\end{outline*}
\subsection{Direction Fields}
\begin{outline*}
\1 Direction fields are a graph of arrows to show the value of y’(x) (the slope of y(x)) for different values of y(x) based on the differential equation as x increases
\2 This is done by first finding each value of y(x) where y’(x) = 0, then testing each region around these values of y(x)
\1 These provide a sketch of the solutions, as well as the long term behavior of y(x) as x increases
\2 The family of solution curves, or set of integral curves, can be drawn based on this to show the graph of every possible solution curve
\2 It also allows determination of how the solution varies based on the value of y(0), and the $\lim_{x \to \infty}y(x)$ based on the value of y(0)
\1 If f’(x) is in terms of both x and f(x), then it must be plotted two dimensionally, first finding the lines where the derivative would be constant
\2 The line such that f’(x) = 0 should be found, then the same techniques can be used to determine the rest of the field, using regional intervals
\end{outline*}
\section{Chapter 2 (Paul) - First Order Equations}
\underline{Note:} Only contains the first four sections
\subsection{Linear Equations}
\begin{outline*}
\1 The equation must be put in the form of $y'(x) + a_1(x)y(x) = a_0(x)$, such that $a_1(x)$ and $a_2(x)$ are continuous
\1 The integrating factor (u(x)) is a function such that $u(x)a_1(x) = u'(x)$
\2 u(x) can be divided by both sides, then it can be changed to (ln(u(x))’ = p(x), which can be integrated to get an equation for u(x)
\1 The integrating factor can then be multiplied by the equation, use the product rule, and integrate to get an equation for y(x)
\1 Constant variables can be changed to simpler constant variables, since they are an unknown constant regardless, as long as they are changed consistently
\end{outline*}
\subsection{Separable Equations}
\begin{outline*}
\1 Separable equations are those in the form of $N(y)\frac{dy}{dx} = M(x)$, which can be multiplied by dx and have the integral taken of it, to a solution in some form
\end{outline*}
\subsection{Exact Equations}
\begin{outline*}
\1 Exact equations are those in the form $M(x, y) + N(x, y)\frac{dy}{dx} = 0$, such that some function, $\Psi (x, y)$,  can be found where $\Psi_x = M(x, y)$ and $\Psi_y = N(x, y)$
\2 Thus, if the derivatives and function of $\Psi$ are continuous, then $M_y = N_x$, while if the latter is not true, it is not exact
\2 By the chain rule, $\frac{d}{dx}(\Psi (x, y(x))) = 0$ is the equation, such that $\Psi (x, y(x)) = c$ is the implicit solution of the equation
\end{outline*}
\subsection{Bernoulli Differential Equations}
\begin{outline*}
\1 Bernoulli equations are those in the form $\frac{dy}{dx} + p(x)y = q(x)y^n$, where p(x) and q(x) are continuous on the interval of validity, and n is a real number
\1 //Finish
\end{outline*}



\section{Chapter 1 (Bretscher) - Linear Equations}
\begin{outline*}
\1 Geometrically, vectors are denoted as an arrow from the origin to the coordinates of the vector, written as $\vec{v}$
\2 It is able to be translated from the origin if necessary (such that if $\vec{v} = (x, y)$ is translated (a, b), it will stretch from (a, b) to (a + x, b + y))
\2 For some infinite set of vectors, it is represented only as the endpoint from the origin for simplicity sake
\2 Vectors are assumed to be column vectors unless otherwise stated, to allow for matrix multiplication
\1 Gauss-Jordan elimination is considered an algorithm, or a finite procedure written in a fixed symbolic vocabulary, with precise instructions, following a discrete set of steps, requiring no insight to complete
\2 On the other hand, it is noted that rounding errors can occur with certain algorithms, limited the ability to automate it
\1 **READ SECTION 1.3**
\end{outline*}
\section{Chapter 2 - Matrices and Systems of Linear Equations}
\underline{Note:} Does not contain Sections 7-8
\subsection{Definition and Notation of Matrices}
\begin{outline*}
\1 Matrices are said to be m x n, with m horizontal rows and n vertical columns, generally denoted by an uppercase variable, where each value is called an entry, represented as an array within brackets
\2 Index notation is such that in an m x n matrix, $A = [a_{ij}] = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots &  a_{mn} \\ \end{bmatrix}$
\2 m x n is called the size of the matrix, while m and n are called the dimensions
\1 Two matrices are said to be equal if they hve the same size, and all cooresponding elements within the matrices are equal
\1 Matrices with a single row or column are called row/column n-vectors, where n is the lenght of the vector, or simply called row/column vectors
\2 The elements of the matrix are called the components of the vector
\2 Regular matrices can thus be broken up into a matrix of m row n-vectors or n column m-vectors
\2 Lists/matrices of vectors must be in the same direction, such that they can be listed in the cooresponding direction
\1 Matrices can be transposed, such that the row and column vectors of the matrix are interchanged, denoted by $A^T$, such that $a_{ji} = a_{ij}^T$
\2 Square matrices, such that $A^T = A$, are called a symmetric matrix
\2 Square matrices, such that $A^T = -A = [-a_{ij}]$, are called a skew-symmetric/anti-symmetric matrix
\3 Skew-symmetric matrices must have the leading diagonal fully zero, by definition, since it maps onto itself
\1 Square matrices are those with the same number of rows and columns
\2 The leading diagonal of a square matrix is the set of all elements, $a_{ii}$, where $1 \leq i \leq n$ for some matrix n x n
\3 The trace of the matrix, denoted tr(A) for matrix A, is the sum of all elements within the leading diagonal
\2 Square matrices are said to be lower triangular if $a_{ij} = 0 \forall i < j$, and upper triangular $\forall i > j$, such that all elements above/below the diagonal are 0
\3 Matrices are called diagonal matrices if they are both lower and upper triangular, able to be denoted as $A = diag(d_1, \cdot, d_n)$, where $d_i = d_{ii}$
\1 Matrix functions are defined as matrices whose elements are single variable functions of $\mathbb{R} \to \mathbb{R}$, able to be similarly written as column or row matrix functions
\end{outline*}
\subsection{Matrix Algebra}
\begin{outline*}
\1 Matrix addition is defined for matrices of the same size, such that if $A = [a_{ij}], B = [b_{ij}], A + B = [a_{ij} + b_{ij}]$
\2 Matrix addition and subtraction are commutative and associative
\1 Matrix scalar multiplication is defined such that $nA = [na_{ij}]$, where $n \in \mathbb{C}$ 
\2 Thus, subtraction is defined as the sum of one matrix and the product of -1 and the other matrix
\2 Scalar multiplication is associative, has a unit property (1A = A), is distributive over matrix addition, and scalar addition is distributive over matrix scalar multiplication
\1 The zero matrix is denoted as $0_{m x n}$, since all elements are zero, such that it is matrix addition identity, and such that zero scalar multiplication and the difference of a matrix and itself returns the zero matrix
\1 For two vectors of the same size, each either row or column, the dot product is defined such that $a \cdot b = a_1b_1 + \cdot + a_nb_n$
\2 The matrix product of a column and a row vector of the same size is extended from this, such that it equals a single element matrix of the dot product value
\2 For some column n-vector and an m x n matrix, the product is extended such that it is the column matrix of the column vector and each row, forming an column m-vector product
\3 As a result, for some m x n matrix multiplied by a column n-vector, the product is the sum of each column matrix multiplied by the next item in the column n-vector ($A_{m x n}c_{n x 1} = c_1a_{1 [m x 1]} + c_2a_{2 [m x 1]} + \dots + c_na_{n [m x 1]}$)
\3 Thus, the column-vector product is found by taking the linear combination of the column m-vectors of A
\4 The linear combination of a list of vectors and a list of scalars of the same size is the sum of the scalar product of each cooresponding item in both lists
\2 For some m x n matrix A and n x p matrix B, the product is defined as the row matrix containing the respective resultant column matrix of each column matrix in B multiplied by A
\3 Thus, $(AB)_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{in}b_{nj}$, forming an m x p resultant matrix
\3 It can also be written in index form, such that $AB_{ij} = \sum_{k=1}^n a_{ik}b_{kj}$ for $1 \leq i \leq m, 1 \leq j \leq p$
\3 Thus, $AB = [Ab_1, Ab_2, \dots, Ab_p]$
\2 Matrix multiplication is noncommutative, but it is associative, and has both left and right distributivity
\2 Matrices multiplied by themselves are represented normally by exponents
\1 The identity matrix of some size n, $I_n$, is represented as a matrix with 1 on the main diagonal, 0 elsewhere, such that $A_{m \times n}I_n = A_{m \times n}$ or $I_mA_{m \times n} = A_{m \times n}$
\2 Identity matrices are also called unit matrices
\1 The transpose operation has properties such that $(A^T)^T = A, (A + C)^T = A^T + C^T, (AB)^T = B^TA^T$
\1 Triangular matrices have the property that the product of two lower/upper square matrices is of the same type of matrix
\2 In addition, the product of two unit lower/upper square matrices is a unit matrix of the same type
\1 All rules that apply to algebraic manipulation of matrices applies equally to both real and complex numbers and functions
\2 Calculus operations are done similarly, taking the derivative or integrating for each term within the matrix
\end{outline*}
\subsection{Systems of Linear Equations Terminology}
\begin{outline*}
\1 The system of linear equations of size m x n is the set of m equations, such that $\{a_{i1}x_1 + \dots + a_{ij}x_j + \dots + a_{in}x_n = b_k\}_{k=1}^m$
\2 $b_i$ are the system constants and $a_{ij}$ are the system coefficients
\3 If $b_{i} = 0 \forall 0 \leq i \leq m$, the system is homogeneous, otherwise nonhomogeneous
\2 The solution to the system is set of ordered n-tuples of real or complex scalars, $(x_1, x_2, \dots, x_n)$, such that the sides of the equation are equal
\3 Since the set of all ordered n-tuples of real numbers is $\mathbb{R}^n$, the solution to the system is a subset of that
\3 Ordered n-tuples within the set of real or complex numbers can also be written as row or column n-vectors
\3 As a result, operations on row/column n-vectors can be applied easily to elements of $\mathbb{R}^n$ or $\mathbb{C}^n$
\2 The matrix of coefficients, A, is the matrix such that the coefficients of each equation form each row, beginning with the first equation
\2 The augmented matrix, $A^\#$, is the matrix of coefficients with an additional final column containing the system constants
\1 For some system of n linear equations, there may be no solution, a single solution, or infinite solutions
\2 Consistent systems are those with at least one solution, while inconsistent systems are those with no solutions
\2 For a set of equal equations, the solution is the equation itself, while otherwise, assuming there is a solution, the solution is some set of a lower dimension (point, line, plane, etc)
\1 For some system of m linear equations, it can be written as the matrix of coefficients multiplied by a column n-vector of the variables of each equation, $x_1, x_2, \dots, x_n$, set equal to the column m-vector of the system constants
\2 As a result, any system of linear equations can also be written as $A\vec{x} = \vec{b}$, where A is the m x n matrix of coefficients, $\vec{x}$ is the column n-vector of variables, and $\vec{b}$ the column m-vector of matrix constants
\3 The column n-vector of variables is also called the vector of unknwons, and the column m-vector of matrix constants is also called the right-hand-side vector
\3 As a result, the vector of unknowns is an element of $\mathbb{R}^n$ and the right-hand-side vector is an element of $\mathbb{R}^m$
\3 Thus, the solution of $A\vec{x} = \vec{B}$ is $\forall\vec{x} \in \mathbb{R}^n$ which satisfy the equation
\3 This also signifies the linear transformation of a n dimensional vector space to an m dimensional vector space, used as an alternative definition of a vector, able to be used to generate mappings such as rotational matrices
\1 For some system of m linear equations, in a column m-vector, $\vec{x}$, the derivative is equal to the derivative of each component equation, and $\vec{x}$ is considered a vector-valued function
\1 The vector formulation of some system of first order linear differential equations is written as $\vec{x}'(t) = A\vec{x}(t) + \vec{b}(t)$
\end{outline*}
\subsection{Elementary Row Operations and Row-Echelon Matrices}
\begin{outline*}
\1 For a system of linear equations, the equations can be permuted/reordered, multiplied by a nonzero constant, and added to another equation within the system without changing the solution
\2 Thus, on the augmented matrix of the system, the rows can be reordered, multiplied by a nonzero constant, or added to another row
\2 These are called the elementary row operations, denoted by $P_{ij}$ for permuting row i and j, $M_i(k)$ for multiplying row i by k, and $A_{ij}(k)$ for adding row i multiplied by k to row j
\3 All row operations are reversible, such that there is some inverse to move to the original matrix ($P_{ji}, M_i(\frac{1}{k}), A_{ij}(-k)$)
\2 For some matrix A, if matrix B can be found by a finite sequence elementary row operations, $A \sim B$, such that the matrices are called row-equivalent
\3 The individual operations within a sequence to get B from A is denoted by $A \sim^n B$ for the nth operation
\1 Row-echelon form of a matrix is such that all rows containing only zeroes are at the bottom of the matrix, the first nonzero element in any nonzero row is a 1 (all rows have a leading 1), and the leading 1 of any row below the first is to the right of the one above
\2 This notably does not mean that the leading 1s have to be in a diagonal, but rather there may be a column without a leading one, such that for a reduced matrix, there does not need to be zeroes there in the other rows above
\2 This allows the back substitution technique to be used, substituting the bottom equation into the one above it, and so forth, to get the solution
\2 Thus, row-echelon form is an upper triangular matrix with all non-zero rows having a leading 1
\1 All matrices have infinite row-equivalent row-echelon matrix which they can be converted to by at least one series of elementary operations
\2 This is generally done by putting a one in the pivot position, or the topmost position of the first non-zero column/pivot column, followed by putting zeroes below, and moving on to the submatrix under and so forth, which then becomes the pivot column
\3 As a result, linear systems are consistent iff the last column of the augmented matrix row-echelon form is not a pivot column
\2 Fractions in general must be avoided for simplicity when getting echelon-matrices
\1 While there are infinite row-equivalent row-echelon matrices for any matrix, all must have the same number of non-zero rows, called the rank of the matrix, denoted by rank(A) for some matrix A
\2 For some matrix of n rows, if there is 1 zero row, there must be some set of constants, $c_1\vec{a}_1 + \dots + c_n\vec{a}_n = 0$, such that each can be written in terms of the others by the elementary operations
\2 The rank of some matrix of size m x n must be $\leq m, n$, by the definition of row-echelon form
\2 Rank(A) = $\vec{0}$ is true iff A is a zero matrix
\1 Reduced row-echelon matrices are a special case such that each leading one has only zeroes in the column otherwise
\2 Each m x n matrix is row-equivalent to a unique reduced row-echelon matrix
\2 This is able to be created by the same procedure, subtracting to produce zeroes above the leading zero after each iteration
\end{outline*}
\subsection{Gaussian Elimination}
\begin{outline*}
\1 Gaussian elimination is the process of converting the augmented matrix to row-echelon form, and using back substitution to solve the system 
\2 Gauss-Jordan elimination is Gaussian elimination when converting it to reduced row-echelon form, not requiring back substitution, denoted for matrix A as rref(A)
\2 If rank(A) = rank($A^\#$), there is a solution to the matrix
\3 If for some matrix, $A_{mxn}$, $n > rank(A^\#)$, (such that there are more columns than rows) there are infinite solutions, setting each surplus variable as a free variable/parameter, such that the rest are bound variables/parameters, in terms of the free ones
\3 The number of free variables for an m x n matrix is equal to n - rank($A^\#$), due to there being n variables total
\3 If n = rank($A^\#$), then there is a single, unique solution
\2 If $rank(A) < rank(A^\#)$, there is no solution, since there is a false statement within the matrix
\2 It can be found that for any matrix, A, $rank(A) = rank(A^T)$
\2 For some square matrix, $A_{nxn}$, if the rank(A) = n, it is considered to be of full rank, while for a non-square matrix, $A_{mxn}$ if either rank(A) = m (full row-rank) or rank(A) = n (full column-rank), it is considered full rank
\1 For systems of equations with imaginary numbers, it is noted that it can be multiplied by the complex conjugate as well as purely real numbers, such that the imaginary terms can be removed
\1 Homogeneous linear systems are those of the form $\vec{A}\vec{x} = \vec{0}$, such that the solution, $\vec{x} = \vec{0}$ is always valid, called the trivial solution
\2 As a result, homogeneous linear systems must either have only the trivial solution, or infinite solution, since each plane must pass through the origin geometrically
\2 In addition, if the number of unknowns, n $>$ m (number of linear equations), there are infinite solutions by necessity, while if m $\geq$ n, it could have a single or infinite solutions based on the  rank
\end{outline*}
\subsection{Inverse of Square Matrices}
\begin{outline*}
\1 The inverse of some matrix, $A_{nxn}$, is the matrix, $A^{-1}_{nxn}$, such that $AA^{-1} = I_n$
\2 This is able to be used to isolate $\vec{x}$ for some system of linear equations, such that for $A\vec{x} = b$, $\vec{x} = A^{-1}b$ is the unique solution to the system for all b in $\mathbb{R}^n$ by the uniqueness of the matrix inverse
\3 In addition, the converse is true, such that if a matrix has a unique solution (such that rank(A) = n), it must be invertible as well
\3 Thus, all homogeneous linear equation systems must have an inverse matrix if they only have the trivial solution
\3 This can be furthered such that if the matrix is consistent for all $\vec{y}$, then it has an inverse matrix
\2 Inverse matrices are unique, such that if $AB = I_n, AC = I_n$, then $B = C = A^{-1}$
\2 Matrices which have an inverse matrix are called invertible or nonsingular, while those which are not are called singular
\2 If A and B are invertible, then $A^{-1}$ is invertible
\3 In addition, AB is invertible, and $(AB)^{-1} = B^{-1}A^{-1}$, as well as $A^T$ is invertible and $(A^T)^{-1} = (A^{-1})^T$
\3 Further, we can say that if AB is invertible, then both A and B are invertible
\2 While for any finite matrix, AB = I, then B is the right inverse of A, and CA = I, then C is the left inverse, such that B = C, for an infinite dimensional matrix, the left and right inverses may not be equal, or one/both may not exist
\1 Inverse matrices can be found by column by column, solving the system of equations such that ${AA^{-1}_t = (I_n)_t}_{t=1}^n$, such that the reduced augmented row-echelon matrix can be solved for, giving the system constants as the column of the inverse matrix
\2 This effectively is modifying $[A I_n]$ such that it changes to the form $[I_n A^{-1}]$, called the Gauss-Jordan Technique
\1 Orthogonal matrices are defined as those such that $A^T = A^{-1}$
\end{outline*}
%\subsection{Elementary Matrices and LU Factorization}
%\begin{outline*}
%\1 Matrices which can be obtained by performing a single elementary row operation on the identity matrix is the elementary matrix
%\2 Thus, it is always a square matrix as a result, able to be denoted purely by the operation used on the identity matrix
%\2 Since all elementary row operations are reversible, there exists an inverse elementary matrix for each
%\2 For some elementary matrix, E, EA is equivalent to performing the E operation on A
%\3 This can be used to reduce any matrix to row-eschlon form, U, such that $U = E_k\dots E_1A$ for some matrix A, created by $A = E_1^-1\dots E_k^-1U$
%\3 Further, for some invertible nxn matrix A, the reduction by this method to reduced row-eschlon form would provide the identity matrix
%\4 As a result though, the same sequence of multiplied elementary operations alone is equal to $A^-1$, and thus $A = E_1^-1\dotsE_k^-1$
%\4 Conversely, all matrices which are the product of elementary matrices are invertible due to the product of invertible matrices being invertible
%\1
%\end{outline*}
%\subsection{The Invertible Matrix Theorem}
%\begin{outline*}
%\1 For some n x n matrix, A, where $A_{ij} \in \mathbb{R}$, it is equivelent to say that A is invertible, rank(A) = n, A can be expressed as a product of elementary matrices
%\2 It is also equivalent to state that A is row-equivalent to $I_n$, Ax = b has a unique solution $\forall b \in \mathbb{R}^n$, and Ax = 0 has only the trivial solution 
%\2 This is called the Invertible Matrix Theorem, summarizing equivalent conditions on invertible matrices
%\end{outline*}
\section{Chapter 3 - Determinants}
\underline{Note:} Does not contain Sections 3-4
\subsection{Definition of the Determinant}
\begin{outline*}
\1 Permutations are an arrangement of discrete objects in a specific order, such that the number of permutations of a set of n objects is n!
\2 The number of inversions of some permutation, $N(\vec{x})$ are the number of pairs of items that are not in natural order, found equal to the number of adjacent swaps needed to restore it
\3 Odd permutations are those with an odd parity, such that the number of inversions is odd, while even permutations are those with an even parity, or an even or zero number of inversions
\3 The parity of a permutation, $\sigma(\vec{x})$ is denoted as 1 if even, -1 if odd, such that $\sigma(\vec{x}) = (-1)^{N(\vec{x})}$
\3 The interchanging of any items of a permutation creates a parity opposite the original
\1 The determinant is based on some value which must be nonzero in order for there to be an inverse matrix, such that the rank is equal to the dimensions of the matrix
\2 $det(A_{1x1}) = a_{11}, det(A_{2x2}) = a_{11}a_{22} - a_{12}a_{21}, det(A_{3x3}) = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}$
\2 $det(A) = \sum_{\vec{p}} \sigma(\vec{p})a_{1(p_1)}a_{2(p_2)}\dotsa_{n(p_n)}$, where $\vec{p}$ is each distinct permutation of \{1, 2, 3, \dots, n\}, said to have an order of n
\3 This can be visually calculated by taking arrows from each of the top and bottom rows of the matrix diagonally to the right, going down from the top, getting positive terms for each, up from the bottom, getting negative terms for each, wrapped to the left
\2 The determinant is also denoted by $|A|$, where A is the matrix
\2 This can be geometrically found to be equivalent to the cross product of $\vec{a}, \vec{b}$ when written as \begin{vmatrix} \vec{i} & \vec{j} & \vec{k} \\ a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \\ \end{vmatrix}
\3 Thus, the area of a parallelogram represented by two vectors is equal to the absolute value of the cross product/determinant similarly
\3 In addition, for some parallelepiped represented by three vectors, the volume is equal to the absolute value of the determinant A, \begin{vmatrix} \vec{a} \\ \vec{b} \\ \vec{c} \\ \end{vmatrix}
\4 Thus, if all three are in the same plane, the determinant would be zero, since the volume is 0, able to test if vectors are coplanar
\1 The Wronskian matrix is the 3x3 matrix of three solutions to some differential equation in the first row, and their first and second derivatives below
\1 The cofactor matrix is the matrix of the discriminants of the submatrices having removed the row and column of that position, alternating positives and negatives, with the main diagonal being positive $(-1^{i + j})$
\2 The minor matrix is the matrix without the sign changes, while the adjoint matrix (adj(A)) is the transpose of the cofactor matrix
\2 For some invertible matrix, A, $A^{-1} = \frac{1}{det(A)}adj(A)$
\end{outline*}
\subsection{Properties of Determinants}
\begin{outline*}
\1 For an upper or lower triangular matrix, $A_{nxn}$, $det(A) = \prod_{i = 1}^n a_{ii}$
\1 For some matrix $A_{nxn}$, for some matrix B obtained by permuting/switching two rows of A, the determinant is det(A) = -det(B)
\2 For some matrix obtained by multiplying on row by a scalar, the determinant is $\frac{1}{k}*det(B)$
\2 For some matrix obtained by adding a multiple of any row of A to some other row, the determinant is det(B)
\1 These elementary row operation multipliers can be used to modify the determinant matrix to upper/lower triangular form, such that it can be calculated easier
\2 This is due to the determinant of a triangular matrix being the product of the diagonal
\1 Furthermore for some square matrices A and B, $det(A^T) = det(A)$ and $det(AB) = det(A)det(B)$, such that the properties true for rows are similarly true for columns/elementary column operations
\2 It follows as a result that $det(A^{-1}) = \frac{1}{det(A)}$
\2 In addition, if A has a row or column of zeroes, det(A) = 0, and if two rows or columns are equal, then det(A) = 0
\2 If $\vec{a}_1, \vec{a}_2, \dots, \vec{a}_n$ are the row/column vectors of A, if B and C are made up of the same vectors except the $i^{th}$, and $a_i = b_i + c_i$, then det(A) = det(B) + det(C), used to break up a matrix of sums by row/column
\end{outline*}
\section{Chapter 4 - Vector Spaces}
\underline{Note:} Does not contain Sections 5, 10-12
\subsection{Vector Space Definition}
\begin{outline*}
\1 Vector spaces are nonempty sets, whose elements are called vectors, which are defined for an addition and scalar multiplication operation, with scalars in the set A, called a vector space over A, generally $\mathbb{R}$ (real vector space) or $\mathbb{C}$ (complex vector space)
\2 It must also be closed under addition and scalar multiplication, such that for elements in the set undergoing those operations, the result is also in the set
\2 It also must have commutative and associative properties of addition, associative properties of scalar multiplication ($c(d\vec{x}) = (cd)\vec{x}$), and distributive properties of scalar multiplication over both vector and scalar addition
\2 It also must have an additive inverse in the set, as well as an identity vector of addition (zero vector, not necessarily 0)
\2 Further, it must have the unit property, such that for a vector scalar multiplied by 1, it is equal to itself
\1 The set $\mathbb{R}^n$ is a real vector space for all $n \geq 1$, and the set $\mathbb{C}^n$ for all $n \geq 1$ can be a real or complex vector space depending on restricting the scalars to either set
\2 In addition, the set of all m x n matrices with real inputs is a real vector space with standard matrix addition and scalar multiplication, denoted $M_{mxn}(\mathbb{R})$ with $M_n(\mathbb{R})$ denoting the real vector space of square matrices
\2 Other vector spaces include $P_n$, the set of all real-valued polynomials of degree $\leq n$ with real coefficients, and $C^k(I)$, the set of all real-valued functions which are $C^k$ on the domain, I
\1 By the definition of vector spaces, it can be proved that for all vector spaces, the zero vector is unique, and the zero property of scalar multiplication is true by a scalar or a vector of zero
\2 The additive inverse of each element is unique, and for each element, the additive inverse is equal to the scalar product of -1 and the element
\2 Further, if the scalar product of some element and scalar is the zero vector, the element and/or the scalar are zero
\end{outline*}
\subsection{Subspaces}
\begin{outline*}
\1 Vector spaces are used to limit the solution set to some vector space of possible unknowns for a differential equations, where the solution set is some subset of the vector space
\2 If the subset is non-empty and a vector space under the same operations as the greater vector space, it is considered a subspace of the greater vector space
\2 Only closure under addition and scalar multiplication must be checked, since all other vector space axioms are inherited from the greater vector space or from the definition of closure
\3 To be closed under scalar multiplication, it is essential that it contains the zero vector to be able to form a subspace, such that it also has that axiom true as well
\1 All vector spaces have a trivial subspace, or a subspace containing only the zero vector
\1 The solution set to any homogeneous system of linear equations ($Ax = 0$) is a subspace of $\mathbb{C}^n$ for any matrix, $A_{mxn}$
\2 The solution set of this equation, as a result, is called the null space of the matrix A, denoted nullspace(A)
\2 For matrices with only real elements, it is usually denoted as a subspace of $\mathbb{R}^n$ instead for simplicity
\2 On the other hand, the solution set to any nonhomogeneous system of linear equations is not a subspace, due to not having the zero vector in the set
\1 The solution set of the homogeneous linear differential equation, $y'' + a_1(x)y' + a_2(x)y = 0$ on some interval I, is a subspace of $C^2(I)$, called the solution space of the equation
\2 It can be found that for a linear differential equation, there is a general solution as the combination of two linearly independent equations which span the solution set
\end{outline*}
\subsection{Spanning Sets}
\begin{outline*}
\1 Linear combinations are expressions of the form, $c_1\vec{v}_1 + c_2\vec{v}_2 + \dots + c_k\vec{v}_k$, for some vector space V, where $v_i \in V, c_i \in \mathbb{C} \forall 1 \leq i \leq k$, such that the resultant expression $\in V$
\2 If all vectors within vector space V can be written as a linear combination of some set of vectors, $\{v_1, \dots, v_k\}$, V is said to be spanned/generated by that set, and the set is aid to span V, or be the spanning set for V
\2 $\mathbb{R}^3$ is spanned by the axis unit vectors, $\vec{i}, \vec{j}, \vec{k}$ as a result, furthered such that any three noncoplanar vectors span it
\2 It can be shown that vectors span a set by proving that there is at least one solution for the equations formed by the linear combination
\3 Thus, for some matrix of vectors, $A_{kx1}, [\vec{v}_1, \dots, \vec{v}_n]$, it spans some vector space V iff the system $A\vec{x} = \vec{v}$ is consistent for all $\vec{v} \in V$
\1 For some set of vectors, v, within a vector space, V, the set of all possible linear combianations of v is a subset of V, denoted span(v)
\2 This can be extended further such that it is noted that span(v) is a subspace of V
\2 span(v) can also be referred to as the subspace of V spanned by v as a result
\2 It is noted that the trivial case of the span states that $span(\emptyset) = {0}$
\end{outline*}
\subsection{Bases and Dimension}
\begin{outline*}
\1 Linearly independent finite sets of vectors in a vector space are those such that the linear combination of the vectors is equal to zero only if the constant multipliers are all zero
\2 Linear dependent finite sets of vectors are those that can equal zero without all zero constants
\2 For some matrix of vectors, A = $[v_1, v_2, \dots, v_k]$ where $\vec{v}_i \in \mathbb{R}^n$, $\{v_1, v_2, \dots, v_k\}$ is linearly independent if $A\vec{x} = 0$ has only a trivial solution
\3 Further, if k $>$ n, then the set is linearly dependent as a result, and if k = n, the set is linearly dependent iff det(A) = 0
\1 Minimal spanning sets are spanning sets containing the minimum number of vectors needed to span the vector space, such that none of the vectors can be formed by the linear combination of the other two
\2 As a result, for all minimal spanning sets within a nontrivial vector space V, the spanning set must be linearly independent
\2 In addition, any nontrivial, finite linearly dependent vector set contains a linearly independent subset with the same linear span as the dependent set
\1 A set of finite vectors in a vector space V is called a basis for V if the vectors are linearly independent and span V, such that it is the minimal spanning set of V
\2 For infinite-dimensional vector spaces, such as $C^n(I), n \geq 1$, it is impossible to find a finite set of vectors which spans the vector space, such that those which have a finite set are called finite-dimensional vector spaces
\2 The standard basis of $\mathbb{R}^n$ are the set of n unit vectors of the Cartesian coordinate system
\1 For any finite-dimensional vector space with a basis of m vectors, any set of more than m vectors is linearly dependent, such that all bases have the same number of vectors
\2 As a result, any spanning set must contain at least m vectors for that vector space
\2 Further, the dimension, dim(V), of the vector space V is the number of vectors in any basis of V
\3 For the trivial vector space, the dimension is defined as 0
\3 It can be found that $dim(\mathbb{R}^n) = n, dim(M_{mxn}(\mathbb{R})) = mn, dim(P_n) = n + 1$ as well
\2 If dim(V) = n, then any set of n linearly independent vectors in V, or any set of n vectors that span V are a basis of V
\3 This allows the dimension of the solution set of a differential equation to allow any linearly independent set of n equations to act as the general solution, spanning the set
\3 As a result, if div(V) = n and S is some set of n vectors in V, if either S is a basis of V, S is linearly independent, or S spans V, then the other two statements are true
\2 For some subspace of vector space V, the dimension of the subspace must be $\leq$ that of V, where if it is equal to the dimension of V, then the subspace must be equal to V
\3 Further, the basis for the subspace is part of the basis for the greater vector space, V, due to there being some set of linearly independent vectors not spanned by the subspace basis, which can be added to form the greater basis, called extending a basis
\end{outline*}
\subsection{Change of Basis}
\begin{outline*}
\1 For any vector within a finite vector space, for each basis of the vector space, each vector can be expressed as a unique linear combination of the basis vectors
\2 The converse is also true, such that if each vector in a vector space can be expressed as a linear combination of a set of vectors, the set is a basis of the space
\1 An ordered basis B of some vector space V is an ordered set, $(\vec{v}_1, \dots, \vec{v}_n)$, such that the n-tuple, $(c_1, c_2, \dots, c_n)$ for some $\vec{v}$ in V, such that $\vec{v} = c_1v_1 + \dots + c_nv_n$, is called the components of $\vec{v}$ relative to the ordered basis B
\2 The column vector of the components, denoted $[\vec{v}]_B$, is called the component vector of  $\vec{v}$ relative to the ordered basis B
\1 For some ordered basis B in vector space V, if $\vec{x}, \vec{y} \in V$, then $[\vec{x} + \vec{y}]_B = [\vec{x}]_B + [\vec{y}]_B$ and $[c\vec{x}]_B = c[\vec{x}]_B$
\1 For some ordered bases B, $\{\vec{v}_1, \dots, \vec{v}_n\}$, and C, $\{\vec{w}_1, \dots, \vec{w}_n\}$, in n-dimensional vector space V, $P_{B \to C} = [[\vec{v}_1]_C, \dots, [\vec{v}_n]_C]$, where $P_{B \to C}$ is the change of base matrix for B to C
\2 As a result, for some $\vec{v}$, $[\vec{v}]_C = P_{B \to C}[\vec{v}]_B$
\2 It follows from this definition that $P_{B \to C}$ and $P_{C \to B}$ are inverses of each other
\3 This can be extended, such that for ordered bases A, B, and C, $P_{A \to C} = P_{B \to C}P_{A \to B}$
\3 In addition, since for the standard basis E, $[\vec{v}]_E = \vec{v}$, such that each free variable has a basis vector with itself once only, $P_{B \to C} = (P_{C \to E})^{-1}P_{B \to E}$
\3 Thus, the inverse of the base matrix can be used to get the change of base matrix from a standard basis, such that $P_{E to B} = [\vec{v}_1 \vec{v}_2 \vec{v}_3]$, when $\vec{v}_n$ are the basis vectors of B in terms of E, such that $P_{B to E}$ is the inverse
\end{outline*}
\subsection{Rank-Nullity Theorem}
\begin{outline*}
\1 rowspace(A) of a matrix is the subspace spanned by the vector set of each row of A, while the colspace(A) is the subspace spanned by the vector set of each column
\2 It is found that for two row-equivalent matrices, the rowspaces of each are equal, and thus the set of nonzero rows of row-echelon form of a matrix is the basis for its rowspace, such that the basis can be found by converting row-echelon
\2 $colspace(A) = rowspace(A^T)$ by the definition of transposing, such that this can be used to find the basis of a colspace
\3 In addition, the row-echelon form of a matrix can be used to produce simplified vectors, where each column with a leading one is corresponds to a column of the original matrix, making up a basis of the colspace by extension of the previous method
\2 It follows that rank(A) = dim(rowspace(A)) = dim(columnspace(A)), although the rowspace and colspace may be in different dimensions
\1 The nullity of some matrix A, denoted nullity(A), is defined as dim(nullspace(A))
\1 The Rank-Nullity Theorem states that for some matrix $A_{mxn}$, rank(A) + nullity(A) = n, such that the nullity of a matrix is equal to the number of free variables in the solution of $A\vec{x} = 0$
\2 Thus, for some homogeneous linear system of equations $A\vec{x} = 0$ where $A_{mxn}$, if rank(A) = n, then there is only the trivial solution, such that nullspace(A) = \{0\}
\3 In addition, if $rank(A) = r < n$, then there are infinite solutions, all found by $\vec{x} = c_1\vec{x}_1 + \dots + c_{n-r}\vec{x}_{n-r}$, where $\{\vec{x}_1, \dots, \vec{x}_{n-r}\}$ is any linearly independent set of n - r solutions
\3 This is called the general solution of the system
\2 For some nonhomogeneous linear system of equations, $A\vec{x} = \vec{b}$ where $A_{mxn}$, if b is not in colspace(A), then the system is inconsistent
\3 If b is in colspace(A), and dim(colspace(A)) = n, then there is a unique solution
\3 If b is in colspace(A) and $dim(colspace(A)) < n$, then there are an infinite solutions, since the set of columns is not a basis, giving infinite combinations to form $\vec{b}$
\3 Since nonhomogeneous systems are not vector spaces, the general solution is of the form, $\vec{x} = c_1\vec{x}_1 + \dots + c_{n-r}\vec{x}_{n-r} + \vec{x}_p$, where $\{\vec{x}_1, \dots, \vec{x}_{n-r}\}$ is a basis for nullspace(A), and $x_p$ is some particular solution
\4 This is due to the difference of two solutions being in the nullspace(A), able to be formed by a linear combination, such that the first is a general solution
\end{outline*}
\section{Chapter 5 - Linear Transformations}
\underline{Note:} Does not contain Sections 2-4, 7, 10
\subsection{Linear Transformation Definition}
\begin{outline*}
\1 For some vector spaces V and W, a mapping T from V into W is some rule which assigns each vector $\vec{v}$ in V to precisely one vector $\vec{w} = T(\vec{v}) \in W$, denoted $T: V \to W$
\2 It is notable that the same vector in W can have multiple vectors in V mapped to it, but for each vector in V, there may only be one mapping in W
\1 Linear transformations T are mappings such that $\forall \vec{u}, \vec{v} \in V, T(\vec{u} + \vec{v}) = T(\vec{u}) + T(\vec{v})$ and $\forall c \in \mathbb{C}, T(c\vec{v}) = cT(\vec{v})$
\2 These are called the linearity properties, V is called the domain of T, and W is called the codomain of T
\2 It is noted that the addition and scalar multiplication operations on each side of the properties are for different vector spaces, and are not required to be the same operations
\2 These properties are used to find the transformation in terms of a different original base without a change of base matrix
\2 Mappings which do not fulfill the linearity properties are called nonlinear transformations
\2 As a result, a mapping is a linear transformation iff $T(c_1\vec{v}_1 + c_2\vec{v}_2) = c_1T(\vec{v}_1) + c_2T(\vec{v}_2)$
\3 Further, since this can be extended for any number of $c_k\vec{v}_k$, for some basis set of those k vectors, it proves that the entire vector space transforms
\2 Further, it is established that $T(\vec{0}_v) = \vec{0}_w$, where each are the respective zero vectors from that vector space, and $T(-\vec{v}) = -T(\vec{v}) \forall \vec{v} \in V$
\1 For some m x n real matrix, A, T: $\mathbb{R}^n \to \mathbb{R}^m$, where T = $A\vec{x}$, then T is a linear transformation, called a matrix transformation
\2 The converses is also true, such that for all linear transformations, T: $\mathbb{R}^n \to \mathbb{R}^m$, it can be expressed as a matrix transformation, T = $A\vec{x}$, where A is the matrix of the transformation of column standard basis vectors in $\mathbb{R}^n$
\3 This is due to the fact that any vector, $\vec{x} = x_1\vec{e}_1 + \dots + x_n\vec{e}_n$, such that by the properties of linear transformations, it is shown to be the same as the transformation
\2 As a result, for the linear transformation T: $\mathbb{R}^n \to \mathbb{R}^m, A_{mxn} = [T(\vec{e}_1), \dots, T(\vec{e}_n)]$, where $\vec{e}_k$ is a standard basis column vector in $\mathbb{R}^n$, is called the matrix of T
\3 This can also be used to model any transformation able to be described by real number standard basis vectors, such as polynomials
\2 A projection transformation is some linear transformation such that $T^2 = T$, such that applying it a second time produces no modification
\end{outline*}
\subsection{The Matrix of a Linear Transformation}
\begin{outline*}
\1 Generalizing the transformation matrix to all vector spaces, for some vector spaces V and W with ordered bases $B = \{\vec{v}_i\}_{i=1}^n$ and $C = \{\vec{w}_i\}_{i=1}^n$, for linear transformation $T: V \to W$, then $[T]^C_B = [[T(\vec{v}_1)]_C, \dots, [T(\vec{v}_n)]_C]$ is the matrix representation of T relative to bases B and C
\2 For some transformation which does not change the vector space or the base, it is called the matrix representation of T relative to the basis B
\2 Thus, the change-of-basis matrix and the $\mathbb{R}^n$ conversion matrix are just special cases of this matrix
\2 As a result, it can be used to determine the transformation by the transformation matrix, by the formula stating that $[T(\vec{v})]_C = [T]^C_B[\vec{v}]_B$, such that the general vector in B can be converted to the corresponding vector in C of the new vector space
\3 Thus, using $[T(\vec{V})]_C$ and C itself, $T(\vec{v})$ can be found for some general vector, $\vec{v}$
\2 This is due to the multiplier acting to transform the given vector first, in terms of the original base, followed by converting it into the correct base rather than leaving it in terms of the transformations of the original base, by the properties of ordered bases and linear transformations
\1 It is found by the change of base equations and linear transformation matrix equations, that $[T]^C_C = P_{B \to C}[T]^B_BP_{C \to B}$, giving a relationship between each of the matrix representations of T relative to some base
\1 Linear transformation matrices can multiplied for the composition of linear transformations, such that for $T_1: U \to V, T_2: V \to W, T_3: U \to W$, then $[T_3]^C_A = [T_2T_1]^C_A = [T_2]^C_B[T_1]^B_A$, providing a single matrix for the total linear transformation
\1 For some linear transformation $T: V \to W$, with ordered bases B for V and C for W, for some $\vec{v} \in V$, then $\vec{v} \in Ker(T)$ iff $[\vec{v}]_B \in nullspace([T]^C_B)$
\2 In addition, for some $\vec{w} \in W$, then $\vec{w} \in Rng(T)$ iff $[\vec{v}]_B \in colspace([T]^C_B)$
\2 The kernel of a transformation T: $V \to W$, Ker(T), is the set of all vectors, $\vec{v} \in V$, such that $T(\vec{v}) = \vec{0}$
\2 The range of a transformation T: $V \to W$, Rng(T), is the subset of W, consisting of all vectors formed by $T(\vec{v}) \forall V \in \vec{v}$
\1 For some linear transformation, $T: V \to W$ with ordered bases B for V and C for W, then T is one-to-one iff $nullspace([T]^C_B) = \{\vec{0}\}$
\2 In addition, T is onto iff $colspace([T]^C_B) = \mathbb{R}^n$, where n = dim(W)
\2 Further, $T: V \to V$ is invertible iff T is one-to-one and onto
\3 As a result, it is found that T is invertible iff $[T]^C_B$ is an invertible matrix for all ordered bases B and C, which is equivalent to $[T]^C_B$ is invertible for some ordered bases B and C in V
\3 Thus, $[T^{-1}]^B_C[T]^C_B = [I]^B_B = I_{dim(B)}$, such that $([T]^C_B)^{-1} = [T^{-1}]^B_C$, allowing $T^{-1}$ to be determined
\2 One-to-one is defined for some transformation, $T: V \to W$, if each distinct element in V is mapped to a distinct element in W, such that for all $\vec{v}_1 \neq \vec{v}_2 \in V$, then $T(\vec{v}_1) = T(\vec{v}_2)$
\2 Onto is defined for some transformation, $T: V \to W$, such that if W = Rng(T)
\end{outline*}
\subsection{The Eigenvalue/Eigenvector Problem}
\begin{outline*}
\1 For some nxn matrix, A, any values of a constant, $\lambda$, such that $A\vec{v} = \lambda\vec{v}$ has non-trivial solutions, are called eigenvalues of A, while the corresponding nonzero vectors are eigenvectors of A
\2 The set of all eigenvalues of A are the spectrum of A
\2 This is based off being used to solve a system of linear first order differential equations, such that $\vec{\frac{d\vec{x}}{dt}} = A\vec{x}(t)$, where \vec{x} is a 2-column vectors, such that $\vec{x}(t) = e^{\lambda t}\vec{v}$
\2 These are also called characteristic values and vectors of A respectively
\2 The matrix can also be viewed as some matrix of a linear transformation, $T: \mathbb{C}^n \to \mathbb{C}^n$, such that $T(\vec{v}) = A\vec{v} = \lambda \vec{v}$, such that it is a transformation that purely stretches the vector
\3 As a result, it is found that if $\vec{v}$ is an eigenvector of A, then $c\vec{v}$ is for all nonzero scalars c
\2 Further, by definition, it can be thought that eigenvalues of A are those such that $(A - \lambda I_n)\vec{v} = 0$, such that it must have nontrivial solutions
\3 As a result, by the fact that lack of invertibility implies infinite solutions, $det(A - \lambda I) = 0$ if $\lambda$ is an eigenvalue of A
\3 $p(\lambda) = det(A - \lambda I_n)$ is called the characteristic polynomial of A of degree n by the definition of the determinant, and $p(\lambda) = 0$ is called the characteristic equation of A
\4 It is noted that the first term, $\lambda^n$ has a coefficient of $\pm 1$, while the second coefficient is tr(A) and the last is det(A), allowing them to be easily found
\3 Thus, it is found that a matrix A is invertible iff 0 is not an eigenvalue of A
\2 Matrices are defective if the number of linearly independent eigenvectors are not the same number as the dimensions of the matrix
\2 Complex eigenvalues and eigenvectors for a real matrix are those found by an imaginary root of the characteristic polynomial, such that it follows that the conjugate of $\lambda$ and $\vec{v}$ are also eigenvalues and vectors
\3 The conjugate of a complex vector is the conjugate of each component within the vector
\3 It is noted that the transformation dilation of an eigenvector is a complex multiplier, such that n vectors are able to span the complex vector space of $\mathbb{C}^n$
\2 For some matrix A with eigenvalue $\lambda$ and eigenvector $\vec{v}$, then $\lambda^2$ is an eigenvalue of $A^2$ and $\vec{v}$ is an eigenvector of $A^2$
\1 The vector subspace spanned by the eigenvectors of a matrix is the eigenspace of the matrix
\1 Complex numbers can be drawn on a complex plane, written by Euler's formula as $z = x + iy = |z|(cos\theta + isin\theta) = |z|e^{i\theta}$, where $z = \sqrt{x^2 + y^2}$
\1 The fundemental equation of a matrix can also be used to get a relationship between some matrix and its exponents, used for simplification
\2 As a result, for some equation $\lambda^n - \lambda = 0$, it is then found that $A^n = A$, such that it can be subsituted in
\3 This can accordingly be substituted into the matrix exponential expansion, to get a form which can be calculated
\2 In addition, for some equation $(\lambda - 1)^2 = 0$, it is found that $(A - I)^2 = 0$, such that for some N, such that $N^2 = 0$, A = I + N
\end{outline*}
\subsection{Diagonalization}
\begin{outline*}
\1 For some equation $\vec{x}' = A\vec{x}$, for some linear change of variables, $\vec{x} = S\vec{y}$, then $y' = S^{-1}AS\vec{y} = B\vec{y}$
\2 Thus, for some square matrices A and B, A and B are similar if  some invertible matrix S exists, such that $B = S^{-1}AS$
\2 Similar matrices can be found to have the same eigenvalues and characteristic polynomial/equations
\1 The simplest diagonal matrix is such that $B = diag(\lambda_1, \dots, \lambda_n)$
\2 For any square matrix A, it is similar to a diagonal matrix iff it is nondefective, such that for some n linearly independent eigenvectors, $\vec{v}_1, \dots, \vec{v}_n$, S = $[\vec{v}_1, \dots, \vec{v}_n]$
\2 Matrices which are similar to a diagonal matrix are called diagonalizable, equivalent to being nondefective
\2 This allows the system of differential equations to be uncoupled, such that $y_1' = \lambda_1y_1$ and $y_2' = \lambda_2 y_2$, such that $\vec{y} = y_1 + y_2$ as the vector span
\3 As a result, for the original system of functions x(t), the identity $\vec{x} = S\vec{y}$ can be used to get the solution
\2 Diagonalization allows matrices to be exponentiated easily, due to the fact that $A^n = SD^nS^{-1}$ and $D^n = diag(\lambda_1^n, \dots, \lambda_k^n)$
\end{outline*}
\subsection{Matrix Exponential Function}
\begin{outline*}
\1 The matrix exponential function is defined for some square matrix $A_{nxn}$ as $e^{At} = I_n + \frac{1}{2!}(At)^2 + \frac{1}{3!}(At)^3 + \dots$, found to converge for all square matrices and values of t to a square matrix
\2 It is also true that for some matrices, A, B, where AB = BA, $e^{(A + B)t} = e^{At}e^{Bt}$
\2 For all square matrices, A, $(e^{At})^{-1} = e^{(-A)t} = e^{-At}$, such that it follows that the inverse of $e^{At}$ is $e^{-At}$
\2 It is also found that for some diagonal matrix, $A = diag(d_1, \dots, d_n)$, then $e^{At} = diag(e^{d_1t}, \dots, e^{d_nt})$
\1 It can be found that for some square nondefective matrix, A, $e^{At} = Se^{Dt}S^{-1}$, where D is the diagonalization, such that the function result can be found by the diagonal
\1 It is notable that for some matrix A at t = 0, $e^{tA} = I, frac{d(e^{tA})}{dt} = A, \frac{d^2(e^{tA})}{dt^2} = A^2$, and so forth
\end{outline*}
\subsection{Jordan-Canonical Form}
\begin{outline*}
\1 The Jordan-Canonical form is an approximation of diagonalization, relying on the matrix being able to have complex entries
\2 This compensates for the lack of enough linearly independent eigenvalues to form the diagonalization matrix
\1 Generalized eigenvectors are found by solving such that $(A - \lambda I)^p \vec{v} = 0$, where p is some positive number
\2 As a result, for some smallest positive integer p, such that for all $\vec{v}$ where $(A - \lambda I)^p \vec{v} = 0$, $(A - \lambda I)^{p-1}\vec{v}$ is a regular eigenvector of A
\2 This can be used to get the remaining linearly independent vector to form a base for a defective matrix
\3 This is based on the fact that for any square matrix A with entries $\in \mathbb{C}^n over \mathbb{C}$, a basis of generalized eigenvectors can be found
\2 For some generalized eigenvector of the lowest possible power, p, $\vec{v}$, the ordered set of the cycle of generalized eigenvectors cooresponding to $\lambda$ is $((A - \lambda I)^{p-1}\vec{v}, \dots, (A - \lambda I)\vec{v}, \vec{v})$
\3 $\vec{v}$ is the initial vector, while $(A - \lambda I)^{p-1}\vec{v}$ is the terminal vector
\3 The only regular eigenvector within the cycle is the terminal vector
\3 The cycle of generalized eigenvectors is linearly independent, proven by multiplying by the matrix to some power for each
\3 Further, for some matrix where $(A - \lambda I)^p = 0_n$, any vector such that $(A - \lambda I)^{p-1}\vec{v} \neq 0$ can be used as the initial vector of a cycle
\1 Jordan blocks corresponding to $\lambda$ is a square matrix of some size, with $\lambda$ throughout the leading diagonal, 1s throughout the superdiagonal (diagonal above the leading), and 0s elsewhere
\2 Since $J - \lambda I$ is a matrix purely of ones in the superdiagonal, it is found that $(J - \lambda I)^n$ is a matrix of ones n diagonals above the superdiagonal
\3 Thus, for some Jordan block matrix, $J_n$, $(J - \lambda I)^n = 0$
\2 It is found that for some Jordan block, J, $e^{tJ} = e^{\lambda t}E$, where E has 1s on the leading diagonal, t on the superdiagonal, $\frac{t^2}{2!}$ on the diagonal above that, and so forth with $\frac{t^{n-1}}{(n-1)!}$ on the furthest
\2 Jordan-Canonical form is thus defined as a square matrix with Jordan blocks of some size on the leading diagonal and zeroes elsewhere
\3 All Jordan-Canonical matrices are upper triangular
\3 It is noted that the size of the individual blocks do not have to be equal
\2 It can be found that every square matrix is similar to a unique Jordan canonical matrix, denoted JCF(A)
\2 It is found that the S multipler matrix to gain Jordan-Canonical form is made up of each cycle of generalized eigenvectors for each eigenvalue, starting from the terminal to initial vectors
\3 The number of Jordan blocks as a result is the number of linearly independent regular eigenvectors, each equal in size to the number of vectors in the cooresponding generalized cycle
\3 The order of the vector sequences must be the same as the order of the Jordan blocks in the JCF(A) matrix as well
\2 As a result, for some Jordan-Canonical matrix, the Jordan blocks can be replaced with the exponential Jordan blocks, E, for each, such that $e^{tA} = SES^{-1}$
\1 For some system of linear differential equations, $\vec{x}' = A\vec{x}$, it can thus be converted to Jordan-Canonical form, allowing the final system to be solved, using back substitution
\end{outline*}
\section{Chapter 7 - Systems of Differential Equations}
\subsection{First Order Linear Systems}
\begin{outline*}
\1 First order linear differential equations over some interval I, are those of the form $\frac{d\vec{x}}{dt} = A(t)_n\vec{x} + \vec{b}(t)$, where A and b are defined over the interval and $\vec{x}$ is a column n-vector of functions
\2 Homogeneous systems are those where $\vec{b} = 0$, while other systems are called nonhomogeneous
\3 The associated homogeneous system for any nonhomogeneous sytem is with the $\vec{b}$ removed from the system
\2 As a result, there are n equations within the system, each equation with a single first order derivative of a single unknown function, purely on the left side of the equation
\1 The solution of a linear differential system on an interval I is an ordered n-tuple of unknown functions $(x_1, \dots, x_n)$
\2 An initial value problem is a set of n auxiliary conditions, one for each unknown function, at some single value, $t_0$, restricting the solution set to a single solution
\1 The solution can be found for a constant coefficient matrix A, by viewing the derivative as a matrix operator, due to being a linear transformation, solving for one of the unknown functions
\2 This then forms a second order linear differential equation, which can be solved, back substituting to get the second function
\2 As a result, most nth order linear differential equations can be written as a first order system of n equations by a change of variables, rewriting each higher derivative except the highest as another function
\3 Thus, the relationship with each function and its lower derivative functions as another equation in the system
\1 It is found that for some system of differential equations, $\frac{d\vec{x}}{dt}(t) = A\vec{x}(t)$ has a solution as $\vec{x}(t) = e^{tA}\vec{x}(0)$ for some matrix of coefficients A
\2 This is due to the fact that for $\frac{dx}{dt}(t) = ax(t)$, then $x = ce^{at} = x(0)e^{at}$
\3 It is thus tested with some $\vec{x} = \vec{v}e^{rt}$, found to be true only if it forms an eigenvector eigenvalue pair of r and $\vec{v}$
\3 As a result, $\vec{x} = S\vec{y}$, where $\vec{y}$ is the column vector of $e^{\lambda_i t}$, forming the diagonalization equation
\2 It is also proven by taking the derivative of the long form of the matrix exponential
\end{outline*}
\subsection{Vector Form of Linear Systems}
\begin{outline*}
\1 The vector space, $V_n(I)$, is the vector space of column n-vectors defined on the interval I
\1 For some set of n column n-vector functions within the space, the Wronskian of the functions is defined as, $W[\vec{x}_1, \dots, \vec{x}_n](t) = det([\vec{x}_1(t), \vec{x}_2(t), \dots, \vec{x}_n(t)])$
\2 It is noted that the Wronskian of column n-vector functions is distinct from the Wronskian of continuous functions previously used
\2 For some point $t_0$ in I, on which the functions are continuous, if $W[X](t_0) \neq 0$, X is linearly independent on I
\1 Vector differential equations are systems of linear first order differential equations, such that they can be written $\vec{x}'(t) = A(t)\vec{x}(t) + \vec{b}(t)$
\2 Thus, the unknown functions are those in the column n-vector function, $\vec{x}$
\2 While the vector space $V_n(I)$ is infinite-dimensional, and thus there is no span for the space, for some $A_n$ matrix function, the subspace solution set of the homogeneous vector differential equation is an n-dimensional subspace
\end{outline*}
\subsection{General Results of First Order Linear Systems}
\begin{outline*}
\1 The Existence and Uniqueness Theorems state that for some initial value problem, $\vec{x}'(t) = A(t)\vec{x}(t) + \vec{b}(t), \vec{x}(t_0) = \vec{x}_0$, where A and B are continuous on some interval I, the problem has a unique solution on I
\1 For some homogeneous vector differential equations ($\vec{x}'(t) = A(t)\vec{x}(t)$, the set of all solutions for some square matrix $A_n$, continuous on interval I, is a vector space of dimension n
\2 This is due to the fact that for some functions $D(\vec{x}) = \vec{x}', T(\vec{x}) = A\vec{x}$, such that the solution set is the kernel of $(D - T)(\vec{x})$
\2 The initial value problem is taken such that for n solutions, $\vec{x_i}(t_0) = e_i$, where $e_i$ is the standard basis vector
\3 Thus, the $W[x_1, \dots x_n](t_0)) = 1$, such that the matrix of solutions to the equation is linearly independent
\3 Since this forms the standard basis vectors, any linear combination of these forms an initial value problem for those scalar multipliers
\2 It follows that for any $x(t)$, it made up of the linear combinations of the standard basis functions, it forms a solution to the homogeneous system, which thus must be the unique solution
\3 This is due to the system being able to be written as $\vec{x}' = A\vec{x}$ with the IVP $\vec{x} = c_1\vec{x}_1 + \dots + c_n\vec{x}_n$
\3 Thus, the solution set is spanned by/has a basis of the n functions that at $t_0$, such that the function formed by the linear combination is the general solution
\1 Thus, for some linear homogeneous first order differential equation, $x' = Ax$, any set of n linearly independent solutions on I is called a fundamental solution set on I
\2 The matrix, $X(t) = [\vec{x}_1, \dots, \vec{x}_n]$ is called the fundamental matrix for the equation as a result
\2 As a result, $\vec{x})t) = \vec{X}(t)\vec{c}$, where $\vec{c} = [c_1, \dots, c_n]^T$
\1 The converse of the Wronkian linear independence definition can be proven, such that if they are linearly independent on an interval, the Wronkian is nonzero for all points in the interval
\2 This is due to the fact that otherwise, for some IVP, $\vec{x}(t_0) = \vec{0}$ for a system, by some linear combination of nonzero functions, there are multiple general solutions
\2 As a result, any point, $t_0$, can have the Wronkian taken to determine if the solution set is linearly independent over the continuous interval
\1 For some nonhomogeneous differential equation, the general solution is that of the homogeneous with a particular solution of the nonhomogeneous added
\end{outline*}
\subsection{Vector Differential Equations: Nondefective Coefficient Matrices}
\begin{outline*}
\1 For the system $\vec{x}' = A\vec{x}$, it becomes easier to solve for some square matrix A of real constants
\2 By the theory of eigenvalues, it was shown that the solution was of the form $\vec{x}(t) = e^{\lambda t}\vec{v}$, where $\vec{v}$ is a constant vector, where $A\vec{v} = \lambda \vec{v}$
\2 As a result, for some nondefective matrix with real eigenvalues, the solutions are linearly independent, such that the general solution can be found as a result
\3 This is proven by the Wronkian for the solution set, assuming linearly independent eigenvectors
\2 The solution can then be taken from the linear combination form, to get a vector-valued function solution form
\1 It can also be found for real-valued eigenvalues by diagonalizing the matrix, such that $A = SDS^{-1}$, such that $y' = Dy$
\2 This allows the general solution to be found from the solution of y, by the identity that $\vec{x} = S\vec{y}$, where S is the matrix of linearly independent eigenvectors
\1 For a matrix with complex eigenvalues, since the matrix is a real matrix, the eigenvalues and vectors are in conjugate pairs
\2 For two real value vector functions, $\vec{u}, \vec{v}$, where the two complex solutions are $\vec{u}(t) \pm i\vec{v}(t)$, then $\vec{u}(t)$ and $\vec{v}(t)$ are real solutions of the system
\3 This is due to the fact that any linear combination of the eigenvectors is a multiple of one of the functions
\2 Thus, the complex solutions can be found by the formulas above, after which the two real valued vector parts are the solutions
\2 It is then found that all real-valued solutions of the complex eigenvectors are linearly independent to each other and to all real eigenvector solutions found
\end{outline*}
\subsection{Vector Differential Equations: Defective Coefficient Matrices}
\begin{outline*}
\1 For some defective matrix A, it requires additional solutions for each eigenvalue, found to be of the form $\vec{x} = e^{\lambda t}(\vec{v} + t\vec{v}_0)$ where $\vec{v}_0$ is the associated eigenvector
\2 This is found to only be the case if $\lambda \vec{v} + \vec{v_0} = A\vec{v}$, providing the system $(A - \lambda I)\vec{v} = \vec{v}_0$
\2 Further, by the definition of an eigenvector, it is found that $\vec{v}$ is a generalized eigenvector of A ($(A - \lambda I)^2\vec{v} = 0$, where $\vec{v}$ is not an eigenvector)
\1 This is extended to higher cycles of generalized eigenvectors, such that for a cycle of n length, n linearly independent solutions are found
\2 For the kth vector in the cycle of generalized eigenvectors, $\vec{x}_k = e^{\lambda t}(\vec{v}_k + t\vec{v}_{k-1} + \dots + \frac{1}{k!}t^k\vec{v}_0)$, done for each vector in the cycle
\3 This assumes that $\vec{v}_0$ is the terminal eigenvector, and thus the only regular eigenvector within the cycle
\end{outline*}
\subsection{Matrix Exponential Solution}
\begin{outline*}
\1 For some system, $\vec{x}' = A\vec{x}$, it is found that $\vec{x}(t) = e^{At}\vec{x}(0)$ by the matrix exponential expansion, unique by the Uniqueness Theorem for IVPs
\1 Since the same system also has a solution of $\vec{x}(t) = X(t)\vec{c}$, where X is the fundamental matrix, for some invertible matrix of constants B, $Y(t) = X(t)B$ is also a fundamental matrix
\2 This is due to the fact that each resultant column is a linear combination of the original functions, proven linearly independent since the columns of B are linearly independent by invertibility
\2 As a result, a fundamental matrix can be found, such that $X(0) = I_n$, called the transition matrix of $\vec{x}' = A\vec{x}$ based at t = 0, written $X_0(t)$
\3 Thus, $\vec{x} = X_0(t)\vec{x}(0)$ for the system, describing the system's transition from t = 0 onward
\3 The transition matrix is able to be found from some matrix X(t), such that $X_0(t) = X(t)X^{-1}(0) = e^{At}$ by the respective solutions
\4 This can be made easier to calculate by the fact that for a matrix function, $(X(0))^{-1} = (X^{-1})(0)$
\1 This can also be used to find n linearly independent solutions from the matrix exponential, such that it is reversed
\2 For all A, $A = \lambda I + (A - \lambda I)$, for any $\lambda$, such that for n linearly independent vectors, $\vec{v}$, e^{At}\vec{v} = e^{I\lambda t}e^{(A - \lambda I)t} = e^{\lambda t}I(\vec{v} + t(A - \lambda I)\vec{v} + \frac{t^2}{2}(A - \lambda I)^2\vec{v} + ...)$
\2 As a result, if some generalized eigenvector can be found for $\lambda$, such that the sequence would end at some point, providing one solution to the function
\end{outline*}
\section{Chapter 6 - Higher Order Linear Differential Equations}
\subsection{General Theory}
\begin{outline*}
\1 Since the derivative is a linear transformation, higher order derivatives can be viewed as by composition of the transformation matrix
\2 As a result, the general linear differential operator of order n can be found, such that $L = D^n + a_1D^{n-1} + \dots + a_n$ and $Ly = y^{(n)} + a_1y^{(n - 1)} + \dots + a_ny$, where $a_i(x), y(x)$
\2 For some nth order linear differential equation, it is thus written as F(x) = Ly, where it is homogeneous if F(x) = 0 on I
\1 The Existence and Uniqueness Theorems state that for some IVP of F(x) = Ly, $y(x_0) = y_0, y'(x_0) = y_1, \dots y^{(n-1)}(x_0) = y_{n-1}, x_0 \in I$, has a unique solution on I if F, $a_1, \dots, a_n$ are continuous on I
\2 Differential equations where F, $a_1, \dots, a_n$ are continuous on I are called regular, assumed to be generally true
\1 For some homogeneous equation, the solution set, S, is the set of all y for the given L, such that Ly = 0, or S = Ker(L)
\2 As a result, all solutions are a subspace of the domain space of the linear transformation, such that $S \subseteq C^n(I)$, called the solution space
\2 For some transformation $C^n(I) \to C^0(I)$, the solution space is of dimension n
\3 This is proven by the fact that the functional Wronskian of linearly independent solutions is nonzero for some point on the interval I, and if linearly independent, also nonzero for all
\4 The converse is also true, such that if it is zero for any point on the interval, it is linearly dependent
\3 The linear combination of n linearly independent solutions as a result is referred to as the general solution to the homogeneous system
\1 Nonhomogeneous linear differential equations have a general solution equal to the associated homogeneous general solution, with any particular solution (specifically solving the nonhomogeneous system) added to it
\2 The associated homogeneous general solution is called the complementary function for the equation
\2 It is noted that for f(x) = Ly and g(x) = Ly, with particular solutions u and v respectively, u + v is a particular solution to f(x) + g(x) = Ly by the properties of linearity
\end{outline*}
\subsection{Constant Coefficient Homogeneous Linear Equations}
\begin{outline*}
\1 For some equation, $y^{(n)} + a_1y^{(n-1)} + \dots + a_ny = F(x)$, it can be written as P(D)y = 0, where P(D) is the polynomial differential operator, $D^n + a_1D^{n-1} + \dots + a_n$
\2 As a result, the polynomial P(r) is the real auxiliary polynomial, with the equation P(r) = 0 as the auxiliary equation
\2 It is found that for two polynomial operators, P(D)Q(D) = Q(D)P(D), such that it is commutative, not true for general linear operators
\3 As a result, the P(D) operator can be factored into a series of polynomial differential operators
\1 It thus true that for some $P(D) = P_1(D)P_2(D)\dotsP_k(D)$, then for all $1 \leq i \leq k$, $P_i(D)y = 0$ is a solution to P(D)y = 0
\2 For some differential equation, $(D-r)^my = 0$ where m is a positive integer and r is a real/complex number, it has m solutions of the form $y = e^{rx}, xe^{rx}, \dots, x^{m-1}e^{rx}$
\2 This set of equations is linearly independent on any interval, acting as a polynomial spanning set of $P_{m-1}(I)$
\2 If r is a complex number, it forms the solution $x^ke^{ax}(cosbx - isinbx) = y$, able to be broken up such that for $y = u + iv$, the conjugate root can be ignored with $y = u$ and $y = v$ as solutions
\2 This is found to form a linearly independent set of n solutions to the differential equations, spanning the solution set
\end{outline*}
\subsection{Undetermined Coefficient Annihilators}
\begin{outline*}
\1 For some linear idfferential equation $P(D)y = F(x)$, if there exists some function, A, such that A(D)F = 0, it forms a homogeneous differential equation A(D)P(D)y = 0
\2 The A(D) operator is said to annihilate F(x), called the annihilator of F(x)
\3 Annihilators thus exist for some F(x) iff y = F(x) is a solution to the homogeneous linear differential equation, A(D)y = 0
\3 As a result, the nonhomogeneous side must be in the form $F(x) = cx^ke^{ax}$, $F(x) = cx^ke{ax}sin(bx)$, $F(x) = cx^ke^{ax}cos(bx)$, or the sum of each, for some nonnegative k and real a, b, and c
\4 For some nonsinusoidal solution, $A(D) = (D - a)^{k+1}$ is an annihalator for all $x^pe^{ax}, p \leq k$
\4 For some sinusoidal solutons, $x^pe^{ax}cos(bx), x^pe^{ax}sin(bx), p \leq k$, $A(D) = (D^2 - 2aD + a^2 + b^2)^{k+1}$
\4 For some linear combination of these forms, the product of the annhilators for each is the overall anhilator
\2 This thus forms a honogeneous linear differential equation, which is able to be solved easily, providing a particular trial solution, in terms of an unknown coefficient
\3 This trial solution will include the complement solution, such that it can be ignored for the particular solution
\3 This can then be plugged into the original equation to find the unknown coefficient within the particular solution
\2 This can then be combined with the complementary solution to the original equation
\3 This is called the method of undetermined coefficients
\1 It is found as a result, that general trial solutions are found by annihilators, such that for $F(x) = cx^ke^{ax}$, $y_p(x) = e^{ax}(A_0 + ... + A_kx^k) if $P(a) \neq 0$
\2 If a is a root of P(x) with multiplicity m, $y_p(x) = x^me^{ax}(A_0 + ... + A_kx^k)$
\2 For $F(x) = cx^ke^{ax}cos(bx)$ or $F(x) = cx^ke^{ax}sin(bx)$, if P(a + ib) $\neq 0$, $y_p = e^{ax}(A_0cosbx + B_0sinbx + xA_1cos(bx) + xB_1sin(bx) + ... + x^kA_kcosbx + x^kB_ksinbx)$
\2 If a + ib is a root of P(x) with multiplicity m, $y_p = x^me^{ax}(A_0cosbx + B_0sinbx + ... + x^kA_kcosbx + x^kB_ksinbx)$
\2 For some F(x) as the sum of the defined functions, the trial solution is the cooresponding sum of trial solutions
\end{outline*}
\subsection{Complex-Valued Trial Solutions}
\begin{outline*}
\1 For some equation $y'' + a_1y' + a_2y = F(x) + iG(x)$, if y(x) = u(x) + iv(x) is a solution, then $u'' + a_1u' + a_2u = F(x)$ and $v'' + a_1v' + a_2v = G(x)
\1 As a result, for some second order linear differential equation, if F(x) has sinusoidal terms, the particular solution can be determined by a complex exponential
\2 The right hand side can be rewritten as a complex exponential, solving for the complex solution, and taking the part cooresponding to the sinusoidal
\2 The trial solution for the right hand side of $ce^{kix}$ is then $A_0e^{kix}$, taking the cooresponding real or imaginary part
\end{outline*}
\subsection{Mechanical System Oscillations}
\begin{outline*}
\1 For some mass spring system at initial position $y_0$ at t = 0, the gravitational force and spring force are opposing, such that ma = mg - k$\delta Y$, where down has a positive sign convention
\2 With a damping force proportional to the velocity, such that $F_d = -cv$, where c is the positive damping constant, and there may be some external force, F(t)
\2 Thus, it is found that $m\frac{d^2y}{dt^2} = mg - k(y_0 + y) - c\frac{dy}{dt} + F(t)$, where $y(0) = y_0, y'(0) = v_0$ as the initial conditions
\3 This is due to setting the system such that $mg = ky_0$, such that the initial position is the point of static equilibrium
\3 This forms the second order linear differential equation, $y'' + \frac{c}{m}y' + \frac{k}{m}y = \frac{F(t)}{m}$
\1 Without the damping constant, the solution is found to be $y(t) = c_1cos\omega_0t + c_2sin\omega_0t$, where $c_1 = A_0cos\phi, c_2 = A_0sin\phi$, such that $y(t) = A_0cos(\omega_0t - \phi)$, called simple harmonic motion
\2 The period of the oscillation is $T = \frac{2\pi}{\omega_0} = 2\pi\sqrt{\frac{m}{k}}$, such that it is constant for the system
\1 If it is damped but without external force, the characteristic polynomial is such that it provides three types of damping, underdamped (complex roots), critically damped, or overdamped (two real roots)
\2 This provides a solution for overdamped of $y = e^{-\frac{ct}{2m}}(c_1e^{\mu t} + c_2e^{-\mu t})$ where $\mu = \frac{\sqrt{c^2 - 4km}{2m}$
\2 The critically damped solution is  $y = e^{-\frac{ct}{2m}}(c_1 + c_2t)$
\2 The underdamped of $y = e^{-\frac{ct}{2m}}(c_1cos(\mu t) + c_2sin(\mu t))$, where $\mu = \frac{\sqrt{4km - c^2}}{2m}$
\3 As a result, the motion is still oscillatory, but not periodic, such that $T = \frac{2\pi}{\mu}
\2 As a result, as time moves towards infinity, the displacement moves towards 0
\1 For a system with an external force, such that it is a nonhomogeneous system, notable only when it is periodic over time, such that $F(t) = F_0cos(\omega t)$, called forced harmonic oscillation
\2 If c = 0 and $\omega_0 \neq \omega$, then $y_p(t) = \frac{F_0}{m(\omega_0^2 - \omega^2)}cos(\omega t)$
\2 If c = 0 and $\omega_0 = \omega$, then $y_p(t) = \frac{F_0}{2m\omega_0^2}tsin(\omega_0 t)$
\2 **FINISH**
\end{outline*}
\subsection{Reduction of Order}
\begin{outline*}
\1 Reduction of order can identify the general solution for any second order linear differential equation from a single solution to the associated homogeneous equation
\2 This is done by substituting $y(x) = y_1u(x)$ into the original equation, to solve for u, giving an equation without a 0th order derivative, allowing it to be substituted to the lower order
\2 It can then generally be separated or solved as a first order linear equation, solving for u(x) to get y(x)
\end{outline*}
\end{document}
